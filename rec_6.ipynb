{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hillelda/ANLP/blob/main/rec_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pip install\n",
        "! pip install datasets\n",
        "! pip install evaluate\n",
        "! pip install accelerate -U\n",
        "! pip install transformers[torch]\n",
        "! pip install torch\n",
        "! pip install peft\n",
        "! pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEdP7gDbXe96",
        "outputId": "de8c28c3-a1e5-43d5-9fd8-05d940756a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (AutoModelForSequenceClassification, AutoTokenizer)\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "T03kjd-FasYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Globals\n",
        "MODE = 'gemma' #@param [\"regular\", \"lora\", \"large\", \"gemma\"]"
      ],
      "metadata": {
        "id": "4zlMW_Kiorco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load model\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "if MODE == 'regular':\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base').cuda()\n",
        "elif MODE == 'lora':\n",
        "  model2 = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base').cuda()\n",
        "elif MODE == 'large':\n",
        "  deberta = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large').cuda()\n",
        "elif MODE == 'gemma':\n",
        "  !huggingface-cli login # ask for token for gemma\n",
        "  gemma = AutoModelForSequenceClassification.from_pretrained('google/gemma-2b').cuda()\n",
        "  gemma_tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b')\n",
        "\n"
      ],
      "metadata": {
        "id": "1EWTygtubK7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(examples['sentence1'], examples['sentence2'], max_length=256, truncation=True, padding='max_length')\n",
        "    return result\n",
        "\n",
        "raw_datasets = load_dataset(\"nyu-mll/glue\", 'mrpc')\n",
        "raw_datasets = raw_datasets.map(preprocess_function,batched=True)\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "\n",
        "# train_dataset = train_dataset.select(range(300)) #training on 5k samples\n",
        "\n",
        "# Set format for PyTorch tensors\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
      ],
      "metadata": {
        "id": "MzdGemd9bGTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Metric\n",
        "metric = evaluate.load(\"accuracy\",)\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     preds = np.argmax(p.predictions, axis=1)\n",
        "#     return metric.compute(predictions=preds, references=p.label_ids)\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    return metric.compute(predictions=pred_labels, references=labels)"
      ],
      "metadata": {
        "id": "LzcqlphYbbwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports for Trainer alternative\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import get_scheduler\n"
      ],
      "metadata": {
        "id": "nNGbtvVgCQ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import get_scheduler\n",
        "\n",
        "\n",
        "# # @title Init trainer\n",
        "# # training_args = TrainingArguments(output_dir='/tmp/', do_eval=True, do_train=True, num_train_epochs=3, per_device_train_batch_size=8, learning_rate =5e-5)\n",
        "# # trainer = Trainer(\n",
        "# #     model=model,\n",
        "# #     args=training_args,\n",
        "# #     train_dataset=train_dataset,\n",
        "# #     eval_dataset=eval_dataset,\n",
        "# #     compute_metrics=compute_metrics,\n",
        "# #     tokenizer=tokenizer,\n",
        "# # )\n",
        "\n",
        "\n",
        "# def train(model, train_dataset, eval_dataset, tokenizer, num_epochs=5, learning_rate=5e-5, batch_size=16):\n",
        "#     model.train()\n",
        "#     model.cuda()\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     optim = Adam(model.parameters(), lr=learning_rate)\n",
        "#     scheduler = get_scheduler(\n",
        "#         \"linear\",\n",
        "#         optim,\n",
        "#         # num_warmup_steps=0,\n",
        "#         num_training_steps=num_epochs * len(train_loader),\n",
        "#         num_warmup_steps = int(0.1 * num_epochs * len(train_loader))\n",
        "#     )\n",
        "#     for epoch in range(num_epochs):\n",
        "#         torch.cuda.empty_cache()\n",
        "#         for batch in tqdm(train_loader):\n",
        "#             optim.zero_grad()\n",
        "#             input_ids = batch['input_ids'].cuda()\n",
        "#             attention_mask = batch['attention_mask'].cuda()\n",
        "#             labels = batch['label'].cuda()\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#             loss = outputs.loss\n",
        "#             loss.backward()\n",
        "#             optim.step()\n",
        "#             scheduler.step()\n",
        "#         print(\"Epoch: \" + str(epoch) + \" - Loss: \" + str(loss.item()))\n",
        "#         model.eval()\n",
        "#         eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "#         all_preds = []\n",
        "#         all_labels = []\n",
        "#         for batch in tqdm(eval_loader):\n",
        "#             input_ids = batch['input_ids'].cuda()\n",
        "#             attention_mask = batch['attention_mask'].cuda()\n",
        "#             labels = batch['label'].cuda()\n",
        "#             with torch.no_grad():\n",
        "#                 outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#             preds = torch.argmax(outputs.logits, dim=1)\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "#         all_preds = np.array(all_preds)\n",
        "#         all_labels = np.array(all_labels)\n",
        "#         accuracy = (all_preds == all_labels).mean()\n",
        "#         print(\"Epochs: \" + str(epoch + 1) + \" - Learning Rate: \" + str(learning_rate) + \" - Batch Size: \" + str(batch_size) + \" - Accuracy: \" + str(accuracy))\n",
        "#     return model, {'accuracy': accuracy}"
      ],
      "metadata": {
        "id": "R2MsO9AUbkNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataset, eval_dataset, tokenizer, num_epochs=3, learning_rate=1e-5, batch_size=8):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            torch.cuda.empty_cache() # helps run on the collab GPU without OOM errors.\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward() # Backpropogation\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        eval_accuracy = 0\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad(): # only evaluating, don't change weights.\n",
        "            for batch in eval_loader:\n",
        "                torch.cuda.empty_cache()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                eval_loss += loss.item()\n",
        "                pred = torch.argmax(outputs.logits, dim=1)\n",
        "                eval_accuracy += (pred == labels).sum().item()\n",
        "\n",
        "        eval_accuracy = eval_accuracy / len(eval_dataset)\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        eval_loss = eval_loss / len(eval_dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}, Eval Loss: {eval_loss:.4f}, LR: {learning_rate}, Batch Size: {batch_size}\")\n",
        "\n",
        "    final_accuracy = eval_accuracy\n",
        "    return model, {'accuracy': final_accuracy}"
      ],
      "metadata": {
        "id": "arAD-PGDgwnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G52O9ToPZbiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train!\n",
        "# Train the model\n",
        "if MODE == 'regular':\n",
        "  trained_model, metrics = train(model, train_dataset, eval_dataset, tokenizer)\n",
        "  metrics"
      ],
      "metadata": {
        "id": "rNUqNUyKbqeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache() # clear the cache befoer next step"
      ],
      "metadata": {
        "id": "MX0quXZDfkmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lora imports\n",
        "import peft\n",
        "from peft import LoraModel, LoraConfig"
      ],
      "metadata": {
        "id": "g3UMkJ3wW4ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model)"
      ],
      "metadata": {
        "id": "i38V27FRXTjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lora model config\n",
        "\n",
        "lora_config_lora = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, #should be about r*2.\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"classification\",\n",
        "    target_modules=['query_proj', 'value_proj'], # learn on target modules with LoRA\n",
        "    # do learn the sequence classification head and the pooler weights\n",
        "    modules_to_save=['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
        ")\n"
      ],
      "metadata": {
        "id": "HWyjL7LwW_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(lora_model)"
      ],
      "metadata": {
        "id": "Oc_ykdxRjLHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train LORA\n",
        "if MODE == 'lora':\n",
        "  lora_model = LoraModel(model2, lora_config_lora, adapter_name=\"default\")\n",
        "  trained_model, metrics = train(lora_model, train_dataset, eval_dataset, tokenizer, learning_rate=1e-4)\n",
        "  metrics"
      ],
      "metadata": {
        "id": "rHJo9u2gZGjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title define deberta\n",
        "if MODE == 'large':\n",
        "  lora_config_deberta = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, #should be about r*2.\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"classification\",\n",
        "    target_modules=['query_proj', 'value_proj'],\n",
        "    modules_to_save=['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
        ")\n",
        "  lora_model_large = LoraModel(deberta, lora_config_deberta, adapter_name=\"default\")"
      ],
      "metadata": {
        "id": "9EWCf3veQ3M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train deberta large\n",
        "if MODE == 'large':\n",
        "  trained_model, metrics = train(lora_model_large, train_dataset, eval_dataset, tokenizer, learning_rate=1e-4)\n",
        "  metrics"
      ],
      "metadata": {
        "id": "I6AEr-a0RbFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemma) # neaded so we can find the layer names in new model."
      ],
      "metadata": {
        "id": "arLJ5QH0SBSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deifne gemma\n",
        "if MODE == 'gemma':\n",
        "  lora_config_gemma = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, #should be about r*2.\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"classification\",\n",
        "    target_modules=['q_proj', 'v_proj'], # other layer names in this model\n",
        "    modules_to_save=['pooler', 'classifier','score.weight']\n",
        "    #'pooler', 'classifier',\n",
        "    #'gate_proj', 'up_proj', 'down_proj'\n",
        ")\n",
        "  lora_model_gemma = LoraModel(gemma, lora_config_gemma, adapter_name=\"default\")"
      ],
      "metadata": {
        "id": "kLP94EVLRvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train gemma\n",
        "if MODE == 'gemma':\n",
        "  trained_model, metrics = train(lora_model_gemma, train_dataset, eval_dataset, gemma_tokenizer, learning_rate=0.0001, batch_size=2)\n",
        "  metrics"
      ],
      "metadata": {
        "id": "OemhYWbjRDIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate\n",
        "# metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "# metrics"
      ],
      "metadata": {
        "id": "crvh2bOKbwPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6DGpInw_tJBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}